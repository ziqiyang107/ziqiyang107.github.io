---
layout: post
title: "DDPM Explained"
date: 2025-11-20
---
## Preliminaries
We start with the notations and definitions:

{% capture th_content %}
Assuming that $p_t(x) > 0$ for all $x \in \mathbb{R}^d$ and $t \in [0, 1]$, then, up to a constant independent of $\theta$, $L_{CFM}$ and $L_{FM}$ are equal. Hence, $\nabla_{\theta} L_{FM}(\theta) = \nabla_{\theta} L_{CFM}(\theta)$
{% endcapture %}

{% include theorem.html 
   type="theorem" 
   title="Theorem" 
   name="FM-CFM Equivalence"
   content=th_content 
%}

<div id='eq-starp'>
$$
\begin{align*}
L_{CFM}(\theta)=\mathbb{E}_{t \sim U[0,1], x_1 \sim q(x_1), x \sim p_t(x|x_1)}\left\|v_t(x)-u_t(x|x_1)\right\|^2  \tag{*'}
\end{align*}
$$
</div>
where $p_t(x|x_1)$ is conditional probability path such that:
<div>
$$
\begin{align*}
p_0(x|x_1) &= p(x)=N(x|0, \vecI) \quad \text{ at time }t=0 \\
p_1(x|x_1) &= N(x|x_1, \sigma_{min}^2 \vecI) \quad \text{ at time $t=1$ concentrates around }x_1 \text{ for some small }\sigma_{min}
\end{align*}
$$
</div>
for a particular sample $x_1$ from the training data. Above expected loss is easy to estimate as long as we know how to sample from $p_t(x|x_1)$ and compute $u_t(x|x_1)$, a good thing is that above [(\*')](#eq-starp) has the same gradients with [(\*)](#eq-star), formally {% include cite.html key="lipman2022flow"%}:


---
{% include bibliography.html keys="chen2014stochastic,lipman2022flow" %}
