---
layout: post
title: "DDPM Explained"
date: 2025-11-20
---
## Preliminaries

{% include cite.html key="ho2020denoising"%} proposed a forward/diffusion process, which is a fixed Markov chain, and a $\textit{reverse process}$ a parameterized Markov chain which is learned to reverse the diffusion process using variational inference. The diffusion process gradually adds (Gaussian) noise to the data in the opposite direction of reverse/sampling process until signal is destroyed, the author also used conditional Gaussian in reverse process transitions, because we can have a relatively simple neural network parameterization. In the following section, we will refer to this paper as DDPM paper. 


Suppose data $\vecx_0 \sim q(\vecx_0)$, where $q(\vecx_0)$ is the true unknown distribution of $\vecx_0$, we use a joint reverse process $p_{\theta}(\vecx_{0:T})$ to model $\vecx_0$ as $p_{\theta}(\vecx_0)=\int p_{\theta}(\vecx_{0:T}) d\vecx_{1:T}$, where $\vecx_1,...,\vecx_T$ are latents of the same dimension $d$ as $\vecx_0$. The reverse process is a learned Markov chain starting at $p(\vecx_T)=N(\vecx_T; \veczero, \vecI)$:

$$
\begin{align*}
p_{\theta}(\vecx_{0:T}) := p(\vecx_T) \prod_{t=1}^T p_{\theta}(\vecx_{t-1}|\vecx_t), \qquad p_{\theta}(\vecx_{t-1}|\vecx_t):=N \big(\vecx_{t-1}; \vecmu_{\theta}(\vecx_t, t), \vecSigma_{\theta}(\vecx_t, t) \big)
\end{align*}
$$

In order to use variational inference, we have approximate posterior called forward/diffusion process

$$
q(\vecx_{1:T}|\vecx_0):=\prod_{t=1}^T q(\vecx_t|\vecx_{t-1})=\prod N(\vecx_t; \sqrt{1-\beta_t}\vecx_{t-1},\beta_t \vecI)
$$

where $\beta_t$'s can be either learned using reparameterization or fixed as hyperparameters, if they are fixed, then there is no learnable parameter in diffusion process $q(\cdot|\cdot)$. 








We start with the notations and definitions:

{% capture th_content %}
Assuming that $p_t(x) > 0$ for all $x \in \mathbb{R}^d$ and $t \in [0, 1]$, then, up to a constant independent of $\theta$, $L_{CFM}$ and $L_{FM}$ are equal. Hence, $\nabla_{\theta} L_{FM}(\theta) = \nabla_{\theta} L_{CFM}(\theta)$
{% endcapture %}

{% include theorem.html 
   type="theorem" 
   title="Theorem" 
   name="FM-CFM Equivalence"
   content=th_content 
%}

<div id='eq-starp'>
$$
\begin{align*}
L_{CFM}(\theta)=\mathbb{E}_{t \sim U[0,1], x_1 \sim q(x_1), x \sim p_t(x|x_1)}\left\|v_t(x)-u_t(x|x_1)\right\|^2  \tag{*'}
\end{align*}
$$
</div>
where $p_t(x|x_1)$ is conditional probability path such that with [(\*)](#eq-star)


---
{% include bibliography.html keys="ho2020denoising,lipman2022flow" %}
