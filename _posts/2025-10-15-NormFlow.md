---
layout: post
title: "The Normalizing Flows"
date: 2025-10-15
---

## Normalizing Flows

The variational inference technique is used in many places in deep learning and statistics, e.g., famous $\textbf{Variational}$ $\textbf{autoencoder (VAE)}$ and as an extension of $\textbf{Expectation-Maximization}$ $\textbf{(EM) algorithm}$, it serves as an approximation of posterior distribution or is used in deriving the lower bound of the marginal log-likelihood of the observed data. We will give the basic setup for obtaining the lower bound of a marginal log-likelihood $\log p_{\vectheta}(\vecx)$:

<div id="eq1">
$$
\begin{align*}
\log p_{\vectheta}(\vecx) &= \log \int p_{\vectheta}(\vecx|\vecz) p(\vecz) d\vecz \\
&= \log \int \frac{q_{\vecphi}(\vecz|\vecx)}{q_{\vecphi}(\vecz|\vecx)} p_{\vectheta}(\vecx|\vecz) p(\vecz) d\vecz \\
&\geq -KL(q_{\vecphi}(\vecz|\vecx)||p(\vecz)) + E_{q}\big[\log p_{\vectheta}(\vecx|\vecz) \big] =: \text{ELBO}   \tag{1}
\end{align*}
$$
</div>
The last inequality can be obtained by Jensen's inequality, and $p_{\vectheta}(\vecx|\vecz)$ is likelihood function and $p(\vecz)$ is the prior distribution latent latent variable $\vecz$. The lower bound is also known as $\textbf{Evidence}$ $\textbf{lower bound}$ $\textbf{(ELBO)}$. Directly maximizing the integral form of the original marginal likelihood is hard, so in variational inference setting, we seek to maximize the ELBO with respect to the model parameter $\vectheta$ and the variational parameter $\vecphi$, and in deep learning framework, $p_{\vectheta}$ and $q_{\vecphi}$ are usually parameterized by neural networks. For example, people usually choose $$q_{\vecphi}(\vecz|\vecx)=\mathcal{N}(\vecz|\vecmu_{\vecphi}(\vecx), \text{diag}(\vecsigma_{\vecphi}^2(\vecx)))$$, and the parameters $\vecmu_{\vecphi}$ and $\vecsigma_{\vecphi}$ of this diagonal Gaussian are parameterized by neural networks. 

Notice that the equality holds in Equation [(1)](#eq1) when the posterior distribution $p_{\vectheta}(\vecz\|\vecx)=q_{\vecphi}(\vecz\|\vecx)$, but usually we don't know the exact form of the posterior distribution, but if we can get a strong family of the approximate distribution $$q_{\vecphi}$$, and we might be able to make the ELBO tighter, then optimizing the ELBO will likely give us a good estimate of $$p_{\vectheta}(\vecx)$$. The approximation distribution $$q_{\vecphi}$$ used in {% include cite.html key="rezende2015variational"%} is called $\textbf{Normalizng flows}$ $\textbf{NF}$. 

If we transform a random variable $\vecz \in \mathbb{R}^d$ using an invertible function $f: \mathbb{R}^d \rightarrow \mathbb{R}^d$ (a function denoted $f^{-1}$, such that $f^{-1} \circ f(\vecz)=\vecz$, then $f$ is invertible and $f^{-1}$ is called the inverse function of $f$), then $$\vecz'=f(\vecz)$$ has the distribution:

$$
\begin{align*}
q(\vecz')=q(\vecz)\left|\text{det}\frac{\partial f^{-1}}{\partial \vecz'} \right|=q(\vecz)\left|\text{det}\frac{\partial f}{\partial \vecz} \right|^{-1},
\end{align*}
$$

where the last equality can be obtained by applying the chain rule of Jacobian matrices and the inverse property of determinants of invertible square matrices. We can further generalize the above equality to length $K$ transformation:
<div id="eq3">
$$
\begin{align*}
\vecz_K &= f_K \circ ...\circ f_2 \circ f_1(\vecz_0)  \\
\log q_K(\vecz_K) &= \log q_0(\vecz_0)-\sum_{k=1}^K \log \left|\text{det}\frac{\partial f_k}{\partial \vecz_{k-1}} \right|  \tag{3}
\end{align*}
$$
</div>
By applying a sequence of invertive mappings $f_k$, we can start from a simple initial distribution $q_0(\vecz_0)$, and then gradually make the distribution of $\vecz_K$ complicated. We will use this idea to first start from some initial distribution $$q_0(\vecz_0|\vecx)$$, and then slowly make $\vecz_k$ more complicated distributed, that hopefully can even cover the posterior $$p_{\vectheta}(\vecz|\vecx)$$.

## Training with Planar Flows
Planar flows are chosen as one of the transformation functions $f$ in {% include cite.html key="rezende2015variational"%}:
<div>
$$
\begin{align*}
f(\vecz)=\vecz+\vecu h(\vecw^T \vecz + b)
\end{align*}
$$
</div>
where $$\vecw \in \mathbb{R}, \vecu \in \mathbb{R}, b \in \mathbb{R}$$ are free parameters. To make the function $f$ invertible, a sufficient condition is: $$h(\cdot)=\text{tanh}(\cdot)$$ and $$\vecw^T \vecu \geq -1$$, both the proof and how this constraint can be implemented can be found in {% include cite.html key="rezende2015variational"%}. The reason of using this type of flow is to circumvent the need to calculate the determinant of above Jacobians, whereas using the planar flows, Equation [(3)](#eq3) becomes:
<div id="eq4">
$$
\begin{align*}
\vecz_K &= f_K \circ ...\circ f_2 \circ f_1(\vecz_0)  \\
\log q_K(\vecz_K) &= \log q_0(\vecz_0)-\sum_{k=1}^K \log|1+\vecu_k^T \psi_k(\vecz_{k-1})|  \tag{4}
\end{align*}
$$
</div>

---
{% include bibliography.html keys="rezende2015variational,silver2016mastering," %}
