---
layout: post
title: "The Normalizing Flows"
date: 2025-10-15
---

## Normalizing Flows

The variational inference technique is used in many places in deep learning and statistics, e.g., famous $\textbf{Variational}$ $\textbf{Autoencoder(VAE)}$ and as an extension of $\textbf{Expectation-Maximization}$ $\text{(EM) algorithm}$, it serves as an approximation of posterior distribution or is used in deriving the lower bound of the marginal log-likelihood of the observed data. We will give the basic setup for obtaining the lower bound of a marginal log-likelihood $\log p_{\vectheta}(\vecx)$:
$$
\begin{align*}
\log p_{\vectheta}(\vecx) &= \log \int p_{\vectheta}(\vecx|\vecz) p(\vecz) d\vecz \\
&= \log \int \frac{q_{\phi}(\vecz|\vecx)}{q_{\phi}(\vecz|\vecx)} p_{\vectheta}(\vecx|\vecz) p(\vecz) d\vecz \\
&\geq -KL(q_{phi}(\vecz|\vecx)||p(\vecz)) + E_{q}\big[\log p_{\vectheta}(\vecx|\vecz) \big]
\end{align*}
$$


<div id="eq1">
$$
\begin{align*}
\vecx_t &= \vecx_{t-1} + h\nabla_{\vecx}\log{p(\vecx_{t-1})} + \sqrt{2h} \vecz_t  \tag{1} \\
\vecz_t &\sim N(0, \vecI)
\end{align*}
$$
</div>

Equation [(1)](#eq1)

{% include cite.html key="yu2022surprising"%}

---
{% include bibliography.html keys="yu2022surprising,silver2016mastering,vaswani2017attention" %}
