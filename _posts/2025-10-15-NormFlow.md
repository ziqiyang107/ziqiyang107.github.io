---
layout: post
title: "The Normalizing Flows"
date: 2025-10-15
---

## Normalizing Flows

The variational inference technique is used in many places in deep learning and statistics, e.g., famous $\textbf{Variational}$ $\textbf{autoencoder(VAE)}$ and as an extension of $\textbf{Expectation-Maximization}$ $\text{(EM) algorithm}$, it serves as an approximation of posterior distribution or is used in deriving the lower bound of the marginal log-likelihood of the observed data. We will give the basic setup for obtaining the lower bound of a marginal log-likelihood $\log p_{\vectheta}(\vecx)$:
<div id="eq1">
$$
\begin{align*}
\log p_{\vectheta}(\vecx) &= \log \int p_{\vectheta}(\vecx|\vecz) p(\vecz) d\vecz \\
&= \log \int \frac{q_{\vecphi}(\vecz|\vecx)}{q_{\vecphi}(\vecz|\vecx)} p_{\vectheta}(\vecx|\vecz) p(\vecz) d\vecz \\
&\geq -KL(q_{\vecphi}(\vecz|\vecx)||p(\vecz)) + E_{q}\big[\log p_{\vectheta}(\vecx|\vecz) \big] =: \text{ELBO}
\end{align*}
$$
</div>
The last inequality can be obtained by Jensen's inequality, and $p_{\vectheta}(\vecx|\vecz)$ is likelihood function and $p(\vecz)$ is the prior distribution latent latent variable $\vecz$. The lower bound is also known as $\textb{Evidence}$ $\textbf{lower bound}$ $\textbf{( ELBO)}$. Directly maximizing the integral form of the original marginal likelihood is hard, so in variational inference setting, we seek to maximize the ELBO with respect to the model parameter $\vectheta$ and the variational parameter $\vecphi$, and in deep learning framework, $p_{\vectheta}$ and $q_{\vecphi}$ are usually parameterized by neural networks. For example, people usually choose $$q_{\vecphi}(\vecz|\vecx)=\mathcal{N}$(\vecz|\vecmu_{\vecphi}(\vecx), diag(\vecsigma_{\vecphi}^2(\vecx)))$, and the parameters $\vecmu_{\vecphi}$ and $\vecsigma_{\vecphi}$ of this diagonal Gaussian are parameterized by neural networks. 

Equation [(1)](#eq1)

{% include cite.html key="yu2022surprising"%}

---
{% include bibliography.html keys="yu2022surprising,silver2016mastering,vaswani2017attention" %}
