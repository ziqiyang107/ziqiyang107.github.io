---
layout: post
title: "The Normalizing Flows"
date: 2025-10-15
---

## Normalizing Flows

The variational inference technique is used in many places in deep learning and statistics, e.g., famous $\textbf{Variational}$ $\textbf{autoencoder (VAE)}$ and as an extension of $\textbf{Expectation-Maximization}$ $\textbf{(EM) algorithm}$, it serves as an approximation of posterior distribution or is used in deriving the lower bound of the marginal log-likelihood of the observed data. We will give the basic setup for obtaining the lower bound of a marginal log-likelihood $\log p_{\vectheta}(\vecx)$:

<div id="eq1">
$$
\begin{align*}
\log p_{\vectheta}(\vecx) &= \log \int p_{\vectheta}(\vecx|\vecz) p(\vecz) d\vecz \\
&= \log \int \frac{q_{\vecphi}(\vecz|\vecx)}{q_{\vecphi}(\vecz|\vecx)} p_{\vectheta}(\vecx|\vecz) p(\vecz) d\vecz \\
&\geq \int q_{\vecphi}(\vecz|\vecx) \log\frac{p_{\vectheta}(\vecx, \vecz)}{q_{\vecphi}(\vecz|\vecx)} =: \text{ELBO}   \tag{1}
\end{align*}
$$
</div>
The last inequality can be obtained by Jensen's inequality, and $p_{\vectheta}(\vecx|\vecz)$ is likelihood function and $p(\vecz)$ is the prior distribution latent latent variable $\vecz$. The lower bound is also known as $\textbf{Evidence}$ $\textbf{lower bound}$ $\textbf{(ELBO)}$. Directly maximizing the integral form of the original marginal likelihood is hard, so in variational inference setting, we seek to maximize the ELBO with respect to the model parameter $\vectheta$ and the variational parameter $\vecphi$, and in deep learning framework, $p_{\vectheta}$ and $q_{\vecphi}$ are usually parameterized by neural networks. For example, people usually choose $$q_{\vecphi}(\vecz|\vecx)=\mathcal{N}(\vecz|\vecmu_{\vecphi}(\vecx), \text{diag}(\vecsigma_{\vecphi}^2(\vecx)))$$, and the parameters $\vecmu_{\vecphi}$ and $\vecsigma_{\vecphi}$ of this diagonal Gaussian are parameterized by neural networks. 

Notice that the equality holds in Equation [(1)](#eq1) when the posterior distribution $p_{\vectheta}(\vecz\|\vecx)=q_{\vecphi}(\vecz\|\vecx)$, but usually we don't know the exact form of the posterior distribution, but if we can get a strong family of the approximate distribution $$q_{\vecphi}$$, and we might be able to make the ELBO tighter, then optimizing the ELBO will likely give us a good estimate of $$p_{\vectheta}(\vecx)$$. The approximation distribution $$q_{\vecphi}$$ used in {% include cite.html key="rezende2015variational"%} is called $\textbf{Normalizng flows}$ $\textbf{NF}$. 

If we transform a random variable $\vecz \in \mathbb{R}^d$ using an invertible function $f: \mathbb{R}^d \rightarrow \mathbb{R}^d$ (a function denoted $f^{-1}$, such that $f^{-1} \circ f(\vecz)=\vecz$, then $f$ is invertible and $f^{-1}$ is called the inverse function of $f$), then $$\vecz'=f(\vecz)$$ has the distribution:

$$
\begin{align*}
q(\vecz')=q(\vecz)\left|\text{det}\frac{\partial f^{-1}}{\partial \vecz'} \right|=q(\vecz)\left|\text{det}\frac{\partial f}{\partial \vecz} \right|^{-1},
\end{align*}
$$

where the last equality can be obtained by applying the chain rule of Jacobian matrices and the inverse property of determinants of invertible square matrices. We can further generalize the above equality to length $K$ transformation:
<div id="eq2">
$$
\begin{align*}
\vecz_K &= f_K \circ ...\circ f_2 \circ f_1(\vecz_0)  \\
\log q_K(\vecz_K) &= \log q_0(\vecz_0)-\sum_{k=1}^K \log \left|\text{det}\frac{\partial f_k}{\partial \vecz_{k-1}} \right|  \tag{2}
\end{align*}
$$
</div>
By applying a sequence of invertive mappings $f_k$, we can start from a simple initial distribution $q_0(\vecz_0)$, and then gradually make the distribution of $\vecz_K$ complicated. We will use this idea to first start from some initial distribution $$q_0(\vecz_0|\vecx)$$, and then slowly make $\vecz_k$ more complicated distributed, that hopefully can even cover the posterior $$p_{\vectheta}(\vecz|\vecx)$$.

## Training with Planar Flows
Planar flows are chosen as one of the transformation functions $f$ in {% include cite.html key="rezende2015variational"%}:
<div>
$$
\begin{align*}
f(\vecz)=\vecz+\vecu h(\vecw^T \vecz + b)
\end{align*}
$$
</div>
where $$\vecw \in \mathbb{R}, \vecu \in \mathbb{R}, b \in \mathbb{R}$$ are free parameters. To make the function $f$ invertible, a sufficient condition is: $$h(\cdot)=\text{tanh}(\cdot)$$ and $$\vecw^T \vecu \geq -1$$, both the proof and how this constraint can be implemented can be found in {% include cite.html key="rezende2015variational"%}. The reason of using this type of flow is to circumvent the need to calculate the determinant of above Jacobians, whereas using the planar flows, Equation [(2)](#eq2) becomes:
<div id="eq3">
$$
\begin{align*}
\vecz_K &= f_K \circ ...\circ f_2 \circ f_1(\vecz_0)  \\
\log q_K(\vecz_K) &= \log q_0(\vecz_0)-\sum_{k=1}^K \log|1+\vecu_k^T \vecpsi_k(\vecz_{k-1})|  \tag{3}
\end{align*}
$$
</div>
where $$\vecpsi_k(\vecz)=h'(\vecw_k^T \vecz + b)\vecw_k$$, and $h'$ denotes the derivative. We will quote authors' original text to state why this is called planar flows:
> ...modifies the initial density $q_0$ by applying a series of contractions and expansions in the direction perpendicular to the hyperplane $\vecx^T \vecz + b = 0$, hence we refer to these maps as planar flows. 

We now seek to maximize the ELBO, which is equivalent to minimize the $-$ELBO, with $\vecz$ replaced by K times planar flow transformed $\vecz_K$:
<div>
$$
\begin{align*}
-\text{ELBO}=L(\vecx, \vectheta, \vecphi)&=\mathbb{E}_{q_{\vecphi}(\vecz_K|\vecx)} \big[\log q_{\vecphi}(\vecz_K|\vecx) - \log p_{\vectheta}(\vecx, \vecz_K) \big] \\
&= \mathbb{E}_{q_0(\vecz_0|\vecx)}\big[\log q_0(\vecz_0|\vecx)-\sum_{k=1}^K \log |1+\vecu_k^T \vecpsi_k(\vecz_{k-1})| - \log p_{\vectheta}(\vecx|\vecz_K) \big] + \text{Const},
\end{align*}
$$
</div>
where Const comes from the expectation on prior $p(\vecz_K)$. We can parameterize the distribution $q_0(\vecz_0\|\vecx)=\mathcal{N}(\vecmu(\vecx)\|\text{diag}(\vecsigma^2(\vecx)))$ parameters using a neural network, as well as the parameters $$\{\vecu_k,\, \vecpsi_k\}_{k=1}^K$$. Now we are ready for the training of the normalizing flow:
1. Parameters: $\vecphi$ variational, $\vectheta$ generative
2. **while** not converged **do**
3. $\quad$ $\vecx$ $\leftarrow$ $$\{\text{sample a mini-batch}\}$$
4. $\quad$ $\vecz_0 \sim q_0(\cdot\|\vecx)$
5. $\quad$ $\vecz_K=f_K \circ f_{K-1} \circ...\circ f_1(\vecz_0)$
6. $\quad$ $\Delta \vectheta \propto -\nabla_{\vectheta} L(\vecx, \vectheta, \vecphi)$
7. $\quad$ $\Delta \vecphi \propto -\nabla_{\vecphi} L(\vecx, \vectheta, \vecphi)$
8. **end while**

During inference, we first sample from prior $\vecz_K \sim p(\vecz_K)=\mathcal{N}(\veczero, \vecI)$, then sample from the trained likelihood/generative model $\vecx \sim p_{\vectheta}(\vecx\|\vecz_K)$. An annealed version of $-$ELBO multiplies a $\beta_t=\min(1, 0.01+t/10000) \in [0,1]$ term in front of the $\log p_{\vectheta}(\vecx\|\vecz_K)$ term, and this modification is said to perform better in {% include cite.html key="rezende2015variational"%}.

## Alternative Form of Normalizing Flows
TODO: However, above paper is still confusing, consulting to "The Principles of Diffusion Models", and finish the rest part


---
{% include bibliography.html keys="rezende2015variational,silver2016mastering," %}
