---
layout: post
title: "The Normalizing Flows"
date: 2025-10-15
---

## Normalizing Flows

The variational inference technique is used in many places in deep learning and statistics, e.g., famous $\textbf{Variational Autoencoder(VAE)}$ and as an extension of $\textbf{Expectation-Maximization (EM) algorithm}$, it serves as an approximation of posterior distribution or is used in deriving the lower bound of the marginal log-likelihood of the observed data. We will give the basic setup for obtaining the lower bound of a marginal log-likelihood $\log p_{\vectheta}(\vecx)$


<div id="eq1">
$$
\begin{align*}
\vecx_t &= \vecx_{t-1} + h\nabla_{\vecx}\log{p(\vecx_{t-1})} + \sqrt{2h} \vecz_t  \tag{1} \\
\vecz_t &\sim N(0, \vecI)
\end{align*}
$$
</div>

Equation [(1)](#eq1)

{% include cite.html key="yu2022surprising"%}

---
{% include bibliography.html keys="yu2022surprising,silver2016mastering,vaswani2017attention" %}
